<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders">
  <meta name="keywords" content="multimodal chatbot">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MammoSAE</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="images/breasticon.png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.27.0/gradio.js"></script>
</head>

<style>
    .section {
    margin-bottom: -30px; /* Adjust this value as needed to reduce the space */
  }
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }
</style>


<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img src="images/breasticon.png" alt="MammoSAE" width="100">
            <h1 class="title is-1 publication-title">MammoSAE: <span class="is-size-2"><span class="is-size-1">I</span>nterpreting  <span class="is-size-1">B</span>reast <span class="is-size-1">C</span>ancer  <span class="is-size-1">C</span>oncept <span class="is-size-1">L</span>earning  with <span class="is-size-1">S</span>parse <span class="is-size-1">A</span>utoEncoders <br> [DeepBreath, MICCAI 2025]</span></h1>
            <div class="is-size-5 publication-authors">
              <!-- First Group of 3 Authors -->
              <div class="author-group">
                  <span class="author-block">
                      <a href="https://scholar.google.com/citations?hl=en&user=g_21RKoAAAAJ" style="color:#f68946;font-weight:normal;">Krishna Kanth Nakka<sup></sup></a>
                  </span>
              </div>

        
          </div>
            <div class="is-size-5 publication-authors">
              Bavaria, Germany<br>
            </div>
      
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2507.15227" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                <span class="link-block">
                  <a href="https://github.com/krishnakanthnakka/MammoSAE.git" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
               
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body" style="text-align: center;">

      <!-- Image -->
      <img src="./images/mammosae.png" 
           alt="Mammo-SAE Framework" 
           style="max-width: 100%; border-radius: 8px; border: 0px solid #ccc;">

      <!-- Caption Below Image -->
      <p style="text-align: justify; margin-top: 12px; font-size: 0.95em; max-width: 900px; margin-left: auto; margin-right: auto;">
        <strong>Mammo-SAE Framework.</strong> The SAE is first trained on patch-level CLIP features 
        <em><strong>x<sub>j</sub> ‚àà ‚Ñù<sup>d</sup></strong></em> at any given layer, projecting them into a high-dimensional, 
        interpretable sparse latent space <em><strong>z ‚àà ‚Ñù<sup>h</sup></strong></em>, and decoding them back for reconstruction. 
        Once trained, the SAE is used to analyze which latent neurons are activated and what semantic information they encode. 
        We also perform targeted interventions in the latent neuron space to assess their influence on downstream label prediction. 
        We observe the learned latents capture diverse regions such as <em>nipple regions, mass regions</em>, and background areas. 
        Red boxes indicate ground-truth mass localization.
      </p>

    </div>
  </div>
</section>




<section class="section" style="background-color:#efeff081">
  <div class="container is-max-desktop">
    <!-- Highlights -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">üî• Highlights</h2>
        <div class="content has-text-justified">
          <ol type="1">
            <li>
  <b>Mammo-SAE Introduction.</b> 
  We propose <code>Mammo-SAE</code>, a sparse autoencoder trained on visual features from Mammo-CLIP, 
  a vision‚Äìlanguage model pretrained on mammogram image‚Äìreport pairs. 
  Mammo-SAE aims to enhance interpretability in breast imaging by learning latent neurons 
  that are human-interpretable. It first identifies highly activated latent neurons 
  and then conducts interventions to understand their causal effects.
</li>
<br>
<li>
  <b>Mammo-SAE Framework.</b> 
  Our framework projects patch-level CLIP features into a high-dimensional sparse latent space 
  designed for human interpretability. We identify monosemantic latent neurons whose activations 
  align with meaningful breast cancer features‚Äîsuch as masses and calcifications‚Äîby finding 
  the most highly activated neurons. We then perform targeted interventions to test how 
  these neurons affect label predictions by selectively activating or deactivating groups of neurons.
</li>
<br>
<li>
  <b>Extensive Evaluation.</b> 
  We visualize spatial activations of top-activated latent neurons, showing that they frequently 
  match clinically relevant regions of interest. Our experiments also reveal confounding factors‚Äî
  such as background patterns‚Äîthat influence model decisions. Furthermore, fine-tuning Mammo-CLIP 
  increases the activation of clinically relevant latent neurons, thus explaining the reasons for performance gains.
</li>

          </ol>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="section" style="background-color: #f7f7f7;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">üß† Mammo-SAE Method Overview</h2>
        <div class="content has-text-justified" style="font-size: 1.05em;">

          <p><strong>Step 1: Feature Extraction</strong></p>
          <p>
            Given an input image <strong>I</strong>, we extract local features from a pretrained Mammo-CLIP model at a specific layer <strong>l</strong>. Each spatial position <strong>j</strong> in the feature map yields a vector <strong>x<sub>l</sub><sup>j</sup> ‚àà ‚Ñù<sup>d</sup></strong>, where <strong>d</strong> is the feature dimension and <strong>N<sub>l</sub> = H<sub>l</sub> √ó W<sub>l</sub></strong> is the number of spatial locations.
          </p>

          <p><strong>Step 2: Sparse Autoencoder Training</strong></p>
          <p>
            The extracted feature <strong>x<sub>l</sub><sup>j</sup></strong> is encoded using weight matrix <strong>W<sub>enc</sub> ‚àà ‚Ñù<sup>d√óh</sup></strong>, passed through a ReLU nonlinearity, and decoded using <strong>W<sub>dec</sub> ‚àà ‚Ñù<sup>h√ód</sup></strong>. The training objective combines reconstruction and sparsity:
          </p>

          <p style="text-align: center; font-family: monospace;">
            L = ‚ÄñW<sub>dec</sub> ¬∑ ReLU(W<sub>enc</sub> ¬∑ x<sup>j</sup>) ‚Äì x<sup>j</sup>‚Äñ<sup>2</sup><sub>2</sub> + Œª‚ÄñReLU(W<sub>enc</sub> ¬∑ x<sup>j</sup>)‚Äñ<sub>1</sub>
          </p>

          <p>
            This encourages the autoencoder to reconstruct input features while activating only a small number of latent neurons, enabling interpretability.
          </p>

          <p><strong>Step 3: Identifying Concept-Neurons</strong></p>
          <p>
            After training, we compute the class-wise mean latent activation <strong>zÃÑ<sup>(c)</sup> ‚àà ‚Ñù<sup>h</sup></strong> over all examples in class <strong>c ‚àà {0, 1}</strong>:
          </p>

          <p style="text-align: center; font-family: monospace;">
            zÃÑ<sup>(c)</sup> = (1 / |D<sub>c</sub>| ¬∑ N<sub>l</sub>) ‚àë<sub>x ‚àà D<sub>c</sub></sub> ‚àë<sub>j=1</sub><sup>N<sub>l</sub></sup> ReLU(W<sub>enc</sub> ¬∑ x<sup>j</sup>)
          </p>

          <p>
            Each latent neuron <strong>t</strong> is scored by its activation <strong>s<sub>t</sub><sup>(c)</sup> = zÃÑ<sub>t</sub><sup>(c)</sup></strong>, and top-scoring neurons are considered concept-aligned.
          </p>

          <p><strong>Step 4: Visualization and Semantic Probing</strong></p>
          <p>
            We visualize input patches that strongly activate each latent neuron. This reveals whether the neuron focuses on meaningful clinical patterns (e.g., masses, calcifications) or irrelevant areas.
          </p>

          <p><strong>Step 5: Latent Interventions</strong></p>
          <p>
            We intervene on the latent activations <strong>z = ReLU(W<sub>enc</sub> ¬∑ x<sup>j</sup>)</strong> by either retaining or suppressing specific neurons:
          </p>

          <ul>
            <li>
              <strong>Top-k Activated:</strong> Keep only the top-k neuron activations:
              <p style="font-family: monospace;">
                z‚Ä≤ = z ‚äô m, &nbsp; where &nbsp; m<sub>i</sub> = 1 if i ‚àà T<sub>k</sub><sup>(0)</sup> ‚à™ T<sub>k</sub><sup>(1)</sup>, else 0
              </p>
            </li>
            <br>
            <li>
              <strong>Top-k Deactivated:</strong> Suppress the top-k neurons:
              <p style="font-family: monospace;">
                z‚Ä≤ = z ‚äô (1 ‚Äì m)
              </p>
            </li>
          </ul>

          <p>
            By comparing the model's outputs before and after intervention, we assess whether the top neurons carry meaningful information or reflect confounding artifacts.
          </p>

        </div>
      </div>
    </div>
  </div>
</section> -->



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">üìä Experiments</h2>
        <div class="content has-text-justified" style="font-size: 1.05em;">

          <p>
            <em>Dataset.</em> We use the VinDr-Mammo dataset 
            <a href="https://vindr.ai/datasets/vindr-mammo" target="_blank" style="text-decoration: underline;">
              [Nguyen et&nbsp;al., 2023]
            </a>, which contains approximately 20,000 full-field digital mammograms from 5,000 patients. 
            The dataset includes expert annotations for breast-specific findings such as <em>mass</em> and 
            <em>suspicious calcification</em>.
          </p>

          <p>
            <em>SAE Training.</em> A single Sparse Autoencoder (SAE) is trained on patch-level features extracted 
            from the fine-tuned Mammo-CLIP model using the Vision-SAEs library. Activations are taken from the final 
            layer of the EfficientNet-B5 backbone trained on the <em>suspicious calcification</em> classification task.
          </p>

          <p>
            To ensure consistency and reduce computational overhead, a shared SAE is used across all experiments 
            rather than training separate SAEs per model. This design enforces a common latent space and allows 
            direct comparison across settings.
          </p>

          <p>
            <em>Hyperparameters.</em> The input feature dimension is <code>d = 2048</code>, with an expansion 
            factor of 8, resulting in a latent dimension <code>h = 16,384</code>. The SAE is trained for 200 epochs 
            with a learning rate of <code>3 √ó 10<sup>‚àí4</sup></code>, sparsity penalty 
            <code>Œª = 3 √ó 10<sup>‚àí5</sup></code>, and a batch size of <code>4096</code>.
          </p>

        </div>
      </div>
    </div>
  </div>
</section>




</div>
</div>
</div>
</div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">

      <!-- Subsection heading -->
      <h3 class="title is-4" style="margin-bottom: 0.5em; text-align: left;">
        Group Interventions on Latent Neurons
      </h3>

      <!-- Short paragraph -->
      <p class="content has-text-justified" style="font-size: 1.05em; margin-bottom: 1em;">
        We investigate the role of class-level latent neurons by selectively activating or deactivating
        the most influential ones. This helps us understand how specific neuron groups contribute to
        downstream predictions and model interpretability.
      </p>

      <!-- Figure -->
      <figure style="text-align: center; margin: 0;">
        <div style="display: flex; justify-content: space-between; gap: 20px; flex-wrap: wrap;">

          <!-- First subfigure -->
          <figure style="margin: 0; flex: 1;">
            <img src="./images/media/topk_on_combined.png" 
                 alt="Suspicious Calcification" 
                 style="width: 100%; border: 1px solid #ccc; border-radius: 8px;">
            <figcaption style="margin-top: 6px; font-size: 0.9em;">
              (a) Top-k class-level latent neurons activated
            </figcaption>
          </figure>

          <!-- Second subfigure -->
          <figure style="margin: 0; flex: 1;">
            <img src="./images/media/topk_off_combined.png" 
                 alt="Benign Mass" 
                 style="width: 100%; border: 1px solid #ccc; border-radius: 8px;">
            <figcaption style="margin-top: 6px; font-size: 0.9em;">
              (b) Top-k class-level latent neurons deactivated
            </figcaption>
          </figure>

        </div>

        <!-- Main figure caption -->
        <figcaption style="margin-top: 15px; font-size: 1em; text-align: left;">
          <b>Figure:</b> Group interventions on class-level latent neurons. Left: Top-k activated intervention‚Äîonly the top-k class-specific neurons are retained, and all others are zeroed out. Right: Top-k deactivated intervention‚Äîthe top-k neurons are zeroed out while the rest remain unchanged. We observe that as few as 10 neurons can significantly affect downstream predictions, highlighting their relevance.
        </figcaption>
      </figure>

    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">

      <!-- Subsection heading -->
      <h3 class="title is-4" style="margin-bottom: 0.5em; text-align: left;">
        Spatial Alignment Between SAE Neurons and Breast Concept Regions
      </h3>

      <!-- Introductory sentence -->
      <p class="content has-text-justified" style="font-size: 1.05em; margin-bottom: 1em;">
        To quantitatively evaluate the spatial alignment between SAE latent activations
        and annotated breast concept regions, we threshold each latent heatmap at the 
        95<sup>th</sup> percentile and extract rectangular bounding boxes to approximate
        predicted concept locations.
      </p>

      <!-- Caption above image -->
      <p style="font-size: 1em; color: #555; margin-bottom: 10px; text-align: left;">
        <b>Figure:</b> Mean Average Precision (mAP) for breast concept localization using the
        top-10 class-level latent neuron activations for class <em>c</em> = 1 across different settings.
      </p>

      <!-- Image -->
      <div style="text-align: center;">
        <img src="./images/media/iou.png" 
             alt="IoU overlap" 
             style="max-width: 100%; border-radius: 8px; border: 1px solid #ccc;">
      </div>

    </div>
  </div>
</section>





<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">

      <!-- Subsection heading -->
      <h3 class="title is-4" style="margin-bottom: 0.5em; text-align: left;">
        Visualization of Class-Level Latent Neurons
      </h3>

      <!-- Introductory sentence -->
      <p class="content has-text-justified" style="font-size: 1.05em; margin-bottom: 1em;">
        We visualize spatial activations of the most influential latent neurons in the fine-tuned  and pretrained 
        models for mass and calcification concept predictions, and seek to understand if they align with clinically meaningful regions.
        We show the ground-truth regions of these concept in the red boxes, however, this information is never used during the training time.

      </p>

      <!-- Caption above image -->
      <h2 class="subtitle has-text-centered" style="margin-bottom: 10px; font-weight: bold;">
        Visualization of class-level latent neurons for Finetuned (Suspicious Calcification) model
      </h2>

      <!-- Image -->
      <div style="text-align: center;">
        <img src="./images/media/latent_visualization_Suspicious_Calcification_finetuned_new.png" 
             alt="Latent visualization of Suspicious Calcification" 
             style="max-width: 100%; border-radius: 8px; border: 1px solid #ccc;">
      </div>

    </div>
  </div>
</section>




<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body" style="text-align: center;">

      <!-- Caption Above Image -->
      <h2 class="subtitle has-text-centered" style="margin-bottom: 10px; font-weight: bold;">
        Visualization of class-level latent neurons for Finetuned (Mass) model
      </h2>

      <!-- Image -->
      <img src="./images/media/latent_visualization_Mass_finetuned_new.png" 
           alt="Latent visualization of Suspicious Calcification" 
           style="max-width: 100%; border-radius: 8px; border: 1px solid #ccc;">

    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body" style="text-align: center;">

      <!-- Caption Above Image -->
      <h2 class="subtitle has-text-centered" style="margin-bottom: 10px; font-weight: bold;">
        Visualization of class-level latent neurons for Pretrained (Suspicious Calcification) model
      </h2>

      <!-- Image -->
      <img src="./images/media/latent_visualization_Suspicious_Calcification_pretrained_new.png" 
           alt="Latent visualization of Suspicious Calcification" 
           style="max-width: 100%; border-radius: 8px; border: 1px solid #ccc;">

    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body" style="text-align: center;">

      <!-- Caption Above Image -->
      <h2 class="subtitle has-text-centered" style="margin-bottom: 10px; font-weight: bold;">
        Visualization of class-level latent neurons for Pretrained (Mass) model
      </h2>

      <!-- Image -->
      <img src="./images/media/latent_visualization_Mass_pretrained_new.png" 
           alt="Latent visualization of Suspicious Calcification" 
           style="max-width: 100%; border-radius: 8px; border: 1px solid #ccc;">

    </div>
  </div>
</section>





  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Citation</h2>
      <pre><code>
  @InProceedings{Nakka_2025_MICCAI,
    author    = {Nakka, Krishna Kanth},
    title     = {Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders},
    booktitle = {Proceedings of the  Deep Breast Workshop on AI and Imaging for Diagnostic and Treatment Challenges in Breast Care, MICCAI 2025},
    month     = {September},
    year      = {2025},
}
  </code></pre>
    </div>
  </section>
  <section class="section" id="Acknowledgement">
    <div class="container is-max-desktop content">
      <h2 class="title">Acknowledgement</h2>
      <p>
        This website is adapted from <a
        href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.  We are thankful to CDA, BIA for releasing the pretrained models.
      </p>
    </div>
  </section>

</body>


</html>

<div style="text-align: center;">
  <a href="https://www.epfl.ch/labs/vita/" target="_blank">
    <img src="images/vita_logo.png" width="200" height="100" alt="VITA Logo">
  </a>
</div>
