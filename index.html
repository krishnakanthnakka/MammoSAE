<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders">
  <meta name="keywords" content="multimodal chatbot">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MammoSAE</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="images/breasticon.png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.27.0/gradio.js"></script>
</head>

<style>
    .section {
    margin-bottom: -30px; /* Adjust this value as needed to reduce the space */
  }
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }
</style>


<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img src="images/breasticon.png" alt="MammoSAE" width="100">
            <h1 class="title is-1 publication-title">MammoSAE: <span class="is-size-2"><span class="is-size-1">I</span>nterpreting  <span class="is-size-1">B</span>reast <span class="is-size-1">C</span>ancer  <span class="is-size-1">C</span>oncept <span class="is-size-1">L</span>earning  with <span class="is-size-1">S</span>parse <span class="is-size-1">A</span>utoEncoders <br> [DeepBreath, MICCAI 2025]</span></h1>
            <div class="is-size-5 publication-authors">
              <!-- First Group of 3 Authors -->
              <div class="author-group">
                  <span class="author-block">
                      <a href="https://scholar.google.com/citations?hl=en&user=g_21RKoAAAAJ" style="color:#f68946;font-weight:normal;">Krishna Kanth Nakka<sup></sup></a>
                  </span>
              </div>

        
          </div>
            <div class="is-size-5 publication-authors">
              Bavaria, Germany<br>
            </div>
      
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2507.15227" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>Main paper</span>
                  </a>
                </span>
                 <span class="link-block">
                  <a href="https://arxiv.org/pdf/2507.15227" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                </span>

                
                <span class="link-block">
                  <a href="https://github.com/krishnakanthnakka/MammoSAE.git" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
               
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body" style="text-align: center;">

      <!-- Image -->
      <img src="./images/mammosae.png" 
           alt="Mammo-SAE Framework" 
           style="max-width: 100%; border-radius: 8px; border: 0px solid #ccc;">

      <!-- Caption Below Image -->
      <p style="text-align: justify; margin-top: 12px; font-size: 0.95em; max-width: 900px; margin-left: auto; margin-right: auto;">
        <strong>Mammo-SAE Framework.</strong> The SAE is first trained on patch-level CLIP features 
        <em><strong>x<sub>j</sub> ‚àà ‚Ñù<sup>d</sup></strong></em> at any given layer, projecting them into a high-dimensional, 
        interpretable sparse latent space <em><strong>z ‚àà ‚Ñù<sup>h</sup></strong></em>, and decoding them back for reconstruction. 
        Once trained, the SAE is used to analyze which latent neurons are activated and what semantic information they encode. 
        We also perform targeted interventions in the latent neuron space to assess their influence on downstream label prediction. 
        We observe the learned latents capture diverse regions such as <em>nipple regions, mass regions</em>, and background areas. 
        Red boxes indicate ground-truth mass localization.
      </p>

    </div>
  </div>
</section>




<section class="section" style="background-color:#efeff081">
  <div class="container is-max-desktop">
    <!-- Highlights -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">üî• Highlights</h2>
        <div class="content has-text-justified">
          <ol type="1">
            <li>
              <b>Mammo-SAE Introduction.</b> We propose <code>Mammo-SAE</code>, a sparse autoencoder trained on visual features from Mammo-CLIP‚Äîa vision-language model pretrained on mammogram image‚Äìreport pairs. Mammo-SAE aims to enhance interpretability in breast imaging by learning latent neurons that correspond to clinical concepts. This approach provides neuron-level insight beyond conventional post-hoc explanations.
            </li>
            <br>
            <li>
              <b>Mammo-SAE Framework.</b> Our framework projects patch-level CLIP features into a high-dimensional sparse latent space, enabling reconstruction and interpretability. We identify monosemantic latent neurons whose activations align with meaningful breast cancer features such as masses and calcifications. We also perform targeted interventions to test how these neurons affect label prediction.
            </li>
            <br>
            <li>
              <b>Extensive Evaluation.</b> We visualize spatial activations of latent neurons, showing that they frequently match clinical regions of interest. Our experiments reveal the presence of confounding factors influencing model decisions. Furthermore, finetuning Mammo-CLIP leads to more distinct latent neuron clusters and improved interpretability and performance.
            </li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" style="background-color: #f7f7f7;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">üß† Mammo-SAE Method Overview</h2>
        <div class="content has-text-justified" style="font-size: 1.05em;">

          <p><strong>Step 1: Feature Extraction</strong></p>
          <p>
            Given an input image \( I \), we extract local features from a pretrained Mammo-CLIP model at a specific layer \( l \). Each spatial position \( j \) in the feature map yields a vector \( \mathbf{x}_l^j \in \mathbb{R}^d \), where \( d \) is the feature dimension and \( N_l = H_l \times W_l \) is the number of spatial locations.
          </p>

          <p><strong>Step 2: Sparse Autoencoder Training</strong></p>
          <p>
            The extracted feature \( \mathbf{x}_l^j \) is encoded using weight matrix \( W_{\mathrm{enc}} \in \mathbb{R}^{d \times h} \), passed through a ReLU nonlinearity \( \phi(\cdot) \), and decoded using \( W_{\mathrm{dec}} \in \mathbb{R}^{h \times d} \). The objective combines a reconstruction loss and sparsity penalty:
          </p>

          <p style="text-align: center;">
            \[
            \mathcal{L} = \left\| W_{\mathrm{dec}} \, \phi(W_{\mathrm{enc}} \mathbf{x}^j) - \mathbf{x}^j \right\|_2^2 + \lambda \left\| \phi(W_{\mathrm{enc}} \mathbf{x}^j) \right\|_1
            \]
          </p>

          <p>
            This encourages the autoencoder to reconstruct input features while activating only a small number of latent neurons, enabling interpretability.
          </p>

          <p><strong>Step 3: Identifying Concept-Neurons</strong></p>
          <p>
            After training, we compute the class-wise mean latent activation \( \bar{\mathbf{z}}^{(c)} \in \mathbb{R}^h \) over all images \( \mathbf{x} \in \mathcal{D}_c \) labeled with class \( c \in \{0, 1\} \):
          </p>

          <p style="text-align: center;">
            \[
            \bar{\mathbf{z}}^{(c)} = \frac{1}{|\mathcal{D}_c| \cdot N_l} \sum_{\mathbf{x} \in \mathcal{D}_c} \sum_{j=1}^{N_l} \phi\left(W_{\mathrm{enc}}\, \mathbf{x}^j\right)
            \]
          </p>

          <p>
            Each latent neuron \( t \) is assigned a score \( s_t^{(c)} = \bar{z}_t^{(c)} \). Neurons are ranked by \( s_t^{(c)} \), and top-scoring ones are considered concept-aligned.
          </p>

          <p><strong>Step 4: Visualization and Semantic Probing</strong></p>
          <p>
            We visualize the input patches that most strongly activate selected neurons. This reveals whether the neuron focuses on meaningful clinical patterns (e.g., masses, calcifications) or spurious areas.
          </p>

          <p><strong>Step 5: Latent Interventions</strong></p>
          <p>
            We perform two types of latent interventions on the encoded activation \( \mathbf{z} = \phi(W_{\mathrm{enc}}\, \mathbf{x}^j) \):
          </p>

          <ul>
            <li>
              <strong>Top-\(k\) Activated:</strong> Retain only the top-\(k\) concept-relevant neuron activations:
              \[
              \mathbf{z}' = \mathbf{z} \odot \mathbf{m}, \quad m_i =
              \begin{cases}
              1, & \text{if } i \in \mathcal{T}_k^{(0)} \cup \mathcal{T}_k^{(1)} \\
              0, & \text{otherwise}
              \end{cases}
              \]
            </li>
            <br>
            <li>
              <strong>Top-\(k\) Deactivated:</strong> Suppress the top-\(k\) neurons and keep all others:
              \[
              \mathbf{z}' = \mathbf{z} \odot (1 - \mathbf{m})
              \]
            </li>
          </ul>

          <p>
            We evaluate the model‚Äôs prediction change before and after intervention to measure the functional importance of the selected neurons.
          </p>

        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">üß™ Experimental Setup</h2>
        <div class="content has-text-justified" style="font-size: 1.05em;">

          <p><strong>Dataset.</strong> We use the VinDr-Mammo dataset&nbsp;
            <a href="https://vindr.ai/datasets/vindr-mammo" target="_blank" style="text-decoration: underline;">
              [Nguyen et al., 2023]
            </a>,
            which contains approximately 20,000 full-field digital mammograms from 5,000 patients. The dataset includes expert annotations for breast-specific findings such as <em>mass</em> and <em>suspicious calcification</em>.
          </p>

          <p><strong>SAE Training.</strong> We train a single Sparse Autoencoder (SAE) on patch-level features extracted from the <strong>fine-tuned Mammo-CLIP</strong> model using the Vision-SAEs library. Specifically, we extract activations from the final layer of the <strong>EfficientNet-B5</strong> backbone trained on the <em>suspicious calcification</em> classification task.
          </p>

          <p>
            To ensure consistency and reduce computational overhead, we use a <strong>shared SAE</strong> across all experiments rather than training separate SAEs per model. This design ensures a common latent space and facilitates direct comparison across settings.
          </p>

          <p><strong>Hyperparameters.</strong> Input feature dimension is set to \( d = 2048 \), and we use an expansion factor of 8, resulting in a latent dimension \( h = 16{,}384 \). The SAE is trained for 200 epochs with a learning rate of \( 3 \times 10^{-4} \), sparsity penalty \( \lambda = 3 \times 10^{-5} \), and batch size of 4096.
          </p>

        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure style="text-align: center;">
        <div style="display: flex; justify-content: center; gap: 20px; flex-wrap: wrap;">
          
          <!-- First subfigure -->
          <figure style="margin: 0;">
            <img src="./images/media/topk_on_combined.png" 
                 alt="Suspicious Calcification" 
                 style="width: 100%; max-width: 350px; border: 1px solid #ccc; border-radius: 8px;">
            <figcaption style="margin-top: 6px; font-size: 0.9em;">
              (a) Topk-k class-level latent neurons Activated
            </figcaption>
          </figure>

          <!-- Second subfigure -->
          <figure style="margin: 0;">
            <img src="./images/media/topk_off_combined.png" 
                 alt="Benign Mass" 
                 style="width: 100%; max-width: 350px; border: 1px solid #ccc; border-radius: 8px;">
            <figcaption style="margin-top: 6px; font-size: 0.9em;">
              (a) Topk-k class-level latent neurons Deactivated
            </figcaption>
          </figure>

        </div>

        <!-- Main figure caption -->
        <figcaption style="margin-top: 15px; font-weight: bold; font-size: 1em;">
          Figure: Group interventions on class-level latent neurons
        </figcaption>
      </figure>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body" style="text-align: center;">

      <!-- Caption Above Image -->
      <h2 class="subtitle has-text-centered" style="margin-bottom: 10px; font-weight: bold;">
        Visualization of class-level latent neurons for Finetuned (Suspicious Calcification) model
      </h2>

      <!-- Image -->
      <img src="./images/media/latent_visualization_Suspicious_Calcification_finetuned_new.png" 
           alt="Latent visualization of Suspicious Calcification" 
           style="max-width: 100%; border-radius: 8px; border: 1px solid #ccc;">

    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body" style="text-align: center;">

      <!-- Caption Above Image -->
      <h2 class="subtitle has-text-centered" style="margin-bottom: 10px; font-weight: bold;">
        Visualization of class-level latent neurons for Finetuned (Mass) model
      </h2>

      <!-- Image -->
      <img src="./images/media/latent_visualization_Mass_finetuned_new.png" 
           alt="Latent visualization of Suspicious Calcification" 
           style="max-width: 100%; border-radius: 8px; border: 1px solid #ccc;">

    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body" style="text-align: center;">

      <!-- Caption Above Image -->
      <h2 class="subtitle has-text-centered" style="margin-bottom: 10px; font-weight: bold;">
        Visualization of class-level latent neurons for Pretrained (Suspicious Calcification) model
      </h2>

      <!-- Image -->
      <img src="./images/media/latent_visualization_Suspicious_Calcification_pretrained_new.png" 
           alt="Latent visualization of Suspicious Calcification" 
           style="max-width: 100%; border-radius: 8px; border: 1px solid #ccc;">

    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body" style="text-align: center;">

      <!-- Caption Above Image -->
      <h2 class="subtitle has-text-centered" style="margin-bottom: 10px; font-weight: bold;">
        Visualization of class-level latent neurons for Pretrained (Mass) model
      </h2>

      <!-- Image -->
      <img src="./images/media/latent_visualization_Mass_pretrained_new.png" 
           alt="Latent visualization of Suspicious Calcification" 
           style="max-width: 100%; border-radius: 8px; border: 1px solid #ccc;">

    </div>
  </div>
</section>





  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Citation</h2>
      <pre><code>
  @InProceedings{Nakka_2025_MICCAI,
    author    = {Nakka, Krishna Kanth},
    title     = {Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders},
    booktitle = {Proceedings of the  Deep Breast Workshop on AI and Imaging for Diagnostic and Treatment Challenges in Breast Care, MICCAI 2025},
    month     = {September},
    year      = {2025},
}
  </code></pre>
    </div>
  </section>
  <section class="section" id="Acknowledgement">
    <div class="container is-max-desktop content">
      <h2 class="title">Acknowledgement</h2>
      <p>
        This website is adapted from <a
        href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.  We are thankful to CDA, BIA for releasing the pretrained models.
      </p>
    </div>
  </section>

</body>


</html>

<div style="text-align: center;">
  <a href="https://www.epfl.ch/labs/vita/" target="_blank">
    <img src="images/vita_logo.png" width="200" height="100" alt="VITA Logo">
  </a>
</div>
